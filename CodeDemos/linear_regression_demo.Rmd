---
title: "Linear Regression Lecture Demo"
author: "Cody Carroll"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees.

```{r}
library(ggplot2)
library(tidyverse)
?trees

trees
treedata=trees
colnames(treedata) = c("Diameter",  "Height", "Volume")

ggplot(treedata, aes(x=Diameter, y=Volume)) + 
  geom_point() +
  geom_smooth(method = "lm")

```

Fit a linear model, add the least squares regression line:
```{r}
treeline = lm(Volume ~ Diameter, data=treedata)
#lm(y ~ x, data = data)
treedata$predicted = predict(treeline)   # Save the predicted values
treedata$residuals = residuals(treeline) # add residuals to the data.frame

treedata


```

```{r}

#add the regression line to the plot with geom_smooth

ggplot(treedata, aes(x=Diameter, y=Volume)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)




```

```{r}
summary(treeline)


#predict the volume for the 10th tree in my dataset
treedata[10,]
-36.94+5.07*(11.2)


treedata[11,]
-36.94+5.07*(11.3)

```





Interpret the meaning of slope and intercept.

Slope: On average, for every 1 in increase in the diameter of a tree, 
we expect its volume to increase by 5.0659 cubic ft. 

```{r}

-36.9435 + 5.0659*12 #predicted vol for a 12in diam tree, 23.8473
-36.9435 + 5.0659*13 #predicted vol for a 13in diam tree, 28.9132

28.9132 - 23.8473


##Q: What is the expected increase or decrease in volume of a tree if I
##increase its diameter by 5 in? 

#slope * change in x  = expected difference in y 

5.0659 * 5 

-36.9435 + 5.0659*12 #predicted vol for a 12in diam tree, 23.8473
-36.9435 + 5.0659*17 #predicted vol for a 17in diam tree, 49.1768

49.1768 - 23.8473


#Q: What is the unit of the slope? 5.0659 cu. ft / in. 
#
```

Intercept:

The intercept is calculated as -36.9435 cu ft.

How do I interpret this value? 


The intercept value is the predicted volume of a tree with diameter 0in. 
This is nonsense!! The intercept is not interpretable in this problem. 



Let's try it with your class data. What's the relationship between shoe size and height for people in this class?
```{r}
survey = read.csv("~/Desktop/repos/master-intro-ds/Data/casestudy1_data.csv")

ggplot(survey, aes(x=heightcm, y=shoesize)) + 
  geom_point() 
surveyline=lm(shoesize ~ heightcm, data = survey)
summary(surveyline)


surveyline$coefficients
surveyline$coefficients[1] + surveyline$coefficients[2] * 160
```

Predict a new value: 
What shoe size would we expect someone who is 180cm (6') to have?

```{r}
summary(surveyline)


-14.85663 + 0.13809 * 180 #predicted shoesize of a 180cm person
```






Doing it by hand:
```{r}
-14.85663+0.13809*(180) #~size 10
```

Doing it by extracting the "coefficients":
```{r}
surveyline$coefficients[1] + surveyline$coefficients[2] * 180
```


Doing it by using the predict function:
```{r}
newdf = data.frame(heightcm = 180)
predict(surveyline, newdf)
#first model, second newdf with x values you want to predict at

xs = c(150,160,170,180,190)
newdf_manyxs = data.frame(heightcm = xs)
yhats = predict(surveyline, newdf_manyxs)
cbind(xs, yhats)
```

Which pair of variables are more strongly related? 
Tree girth & volume? Or student height & shoe size?

```{r}
gg1 = ggplot(treedata, aes(x=Diameter, y=Volume)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

gg2 = ggplot(survey, aes(x=heightcm, y=shoesize)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

library(ggpubr)
ggarrange(gg1, gg2)
```

The strength of the /linear/ relationship is measured by the $R^2$ statistic:

```{r}
summary(treeline) #0.9353
summary(surveyline) #0.5955

summary(treeline)$r.squared
summary(surveyline)$r.squared

#R^2 is always locked between 0 and 1 (for linear regression.)
#R^2 closer to 0: very weak linear relationship between x and y.
#R^2 is closer to 1: very strong linear relationship between x and y. 


#Exercise: 
#In your survey data find a pair of variables x and y such that the R^2 value is nearly 1. 

library(GGally)
ggpairs(survey[,10:15])


ggplot(survey, aes(x = heightcm, y = heightin)) + 
  geom_point()



#How to remove that outlier?? 

#manual filtering
survey[survey$heightin<20, ]
survey_no_outlier = survey[-48, ]

#idx based filtering
idx = which(survey$heightin<20)
survey_no_outlier = survey[-idx, ]

#using dplyr functions
survey_no_outlier = survey %>% filter(heightin>20)

ggplot(survey_no_outlier, aes(x = heightcm, y = heightin)) + 
  geom_point()


mod = lm(heightin ~ heightcm, data = survey_no_outlier)
summary(mod)


#Exercise: find a pair of x and y variables where R^2 is very close to 0. 


ggplot(survey, aes(x = X, y = number)) + 
  geom_point()

randomlm = lm(number ~ shoesize, data = survey)
summary(randomlm)
```

$R^2$ is also called the "coefficient of determination."

It can be interpreted as the fraction of variation in $y$ explained by the regression line.

$R^2$ has a somewhat technical definition; has to do with residuals and variance of $y$. Not our focus here. 

#FYI
# $R^2 = 1 - SSE/SST$


# Residual plot
The residual plot shows the x-values vs. the residuals:
```{r}
ggplot(treedata, aes(x=Diameter, y=residuals))  + 
  geom_hline(aes(yintercept = 0), color="blue") + 
  geom_point() 


survey$residuals = residuals(surveyline)
ggplot(survey, aes(x=heightcm, y=residuals))  + 
  geom_hline(aes(yintercept = 0), color="blue") + 
  geom_point() 
```

If you're getting an error, make sure you've actually added the residuals to your original data frame!! Otherwise ggplot cannot find them.


Let's visualize residuals on the original scatter plot for the tree data.
```{r}
ggplot(treedata, aes(x=Diameter, y=Volume)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  geom_segment(aes(xend = Diameter, yend = predicted), color = "red")
```



How does this relate back to our original linear regression plot?

#key takeaway when you make a residual plot
Pattern in residuals -> bad. Some assumption has been violated. It means linear regression (in its current form) isn't appropriate.


Example of this:
Women's heights and weights
```{r}
?women

womendata=women
ggplot(womendata, aes(x=height, y=weight)) +  
  geom_point()

womenline=lm(weight~height, data=women)
ggplot(womendata, aes(x=height, y=weight)) +  
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 
```

Hmm.. seems not so bad. but let's see residuals to check:
```{r}
womendata$residuals=residuals(womenline)

ggplot(womendata, aes(x=height, y=residuals)) +  
  geom_hline(aes(yintercept = 0), color="blue") + 
  geom_point()
```
Clear pattern -> linear regression not appropriate/
What does this mean?
The linear regression is making systematic errors in its predictions.
We should use something other than a line to model the relationship!


## Beware overrelying on $R^2$:
```{r}
summary(womenline)
```
High $R^2$ ! $R^2=$.991 ... even though we have pattern in residuals.
Having a high $R^2$ doesn't necessarily mean the linear regression is the best model choice.

In other words, there probably is some other nonlinear relationship out there that would be better at explaining the relationship between women's height & weights.

Limitations of regression: **ALWAYS LOOK AT THE PLOT**!

Cautionary tale: Anscombe's quartant dataset
```{r}
g1 = ggplot(anscombe, aes(x=x1, y=y1)) + 
  geom_point() 
g2 = ggplot(anscombe, aes(x=x2, y=y2)) + 
  geom_point() 
g3 = ggplot(anscombe, aes(x=x3, y=y3)) + 
  geom_point() 
g4 = ggplot(anscombe, aes(x=x4, y=y4)) + 
  geom_point() 


#btw, gridExtra package will let us see all at once
library(gridExtra)
grid.arrange(g1, g2, g3, g4, nrow = 2)
```

Let's see the linear regression outputs though:

```{r}
mod1=lm(y1~x1, data=anscombe)
mod2=lm(y2~x2, data=anscombe)
mod3=lm(y3~x3, data=anscombe)
mod4=lm(y4~x4, data=anscombe)
```

Statistics are the same:
```{r}
summary(mod1)
summary(mod2)
summary(mod3)
summary(mod4)
```

Means of x's and y's are same:
```{r}
colMeans(anscombe)
```
sd's of x's and y's same:
```{r}
apply(anscombe, 2, sd)
```

# OTHER CAUTIONARY TALES IN REGRESSION


1920s car data- stopping distances for brakes:

```{r}
?cars

##>>>Note that the data were recorded in the 1920s.<<<
str(cars)
ggplot(cars, aes(x=speed, y=dist)) + geom_point()

#fit the least squares line
carsline=lm(dist~speed, data=cars)
summary(carsline)
ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE)
```

Now let's say I wanted to estimate the distance taken to stop for a car going 120mph.

```{r}
-17.5791+3.9324*120

newdata=data.frame(speed=120, dist=454.3089)

ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() + geom_point(data=newdata) + 
  geom_smooth(method = "lm", se = FALSE, fullrange=T)
```

Ok.... but does this make sense? Using your **common sense**!

What kind of cars were going 120mph in 1920?????
Not a sensible question to ask!

Additionally, we have no data near the point of a car going 120 mph, so we have no concrete evidence that the linear trend would continue, i.e. some other trend may occur once we get to extreme speeds.

These problems are some of the issues associated with *extrapolation*.

Extrapolation occurs when you try to use an estimated regression equation to predict a new y value for x values outside of the range of of the sample data used to determine the regression equation.


Another extrapolation example... from the Anscombe dataset you saw before:

```{r}
ggplot(anscombe, aes(x=x2, y=y2)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE)

summary(mod2)
```

Predict a value for x = 20:
```{r}
3+.5*20

newanscombe=data.frame(x2=20, y2=13)

ggplot(anscombe, aes(x=x2, y=y2)) + 
  geom_point() + geom_point(data = newanscombe) + 
  geom_smooth(method = "lm", se = FALSE, fullrange = T, color = "red")

```

Probably not right... more likely to be something like:
```{r}
ggplot(anscombe, aes(x=x2, y=y2)) + 
  geom_point() + geom_point(data = newanscombe) + 
  geom_smooth(method = "lm", se = FALSE, fullrange = T,  color = "red") + 
  geom_smooth(method = "lm", formula= y ~ poly(x, 2, raw=TRUE), se = FALSE, fullrange = T) 

quadanscombe=data.frame(x2=20, y2=-1.0598)

ggplot(anscombe, aes(x=x2, y=y2)) + 
  geom_point() + geom_point(data = newanscombe) + 
  geom_smooth(method = "lm", se = FALSE, fullrange = T,  color = "red") + 
  geom_smooth(method = "lm", formula= y ~ poly(x, 2, raw=TRUE), se = FALSE, fullrange = T) +
  geom_point(data = quadanscombe) 
```

Very very different predictions for x=20!! 
Linear regression is not always the answer. 
How could we have known that the linear model was not appropriate, before even trying to extrapolate?

Residual plot!!!
```{r}
anscombedata=anscombe
ansline=lm(y2~x2, data=anscombedata)
anscombedata$residuals=residuals(ansline)

ggplot(anscombedata, aes(x=x2, y=residuals))  + 
  geom_hline(aes(yintercept = 0), color="blue") + 
  geom_point() 
```

Clear pattern! Systematic error is present in the LSRL predictions.

Btw if you were wondering how I got that point, (20, -1.0598), I used polynomial regression-- a more advanced generalization of linear regression:
```{r}
lm(y2 ~ poly(x2, 2, raw=TRUE), data=anscombe)
-5.9958+2.7808*20-0.1267*(20)^2
```

Bonus video: This is what might happen to you if you try to extrapolate. :P  
https://vimeo.com/212731897
```

